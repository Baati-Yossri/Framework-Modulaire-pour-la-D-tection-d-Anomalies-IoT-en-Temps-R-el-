# Documentation des Modules IoT

Ce dossier contient les composants modulaires du framework de d√©tection d'anomalies IoT. Chaque sous-dossier g√®re une partie sp√©cifique du pipeline de donn√©es.

## üìÇ Structure des Modules

| Module | Fichier Principal | Description |
| :--- | :--- | :--- |
| **Ingestion** | `ingestion/producer.py` | Simule des capteurs IoT et envoie les donn√©es vers Kafka. |
| **Machine Learning** | `ml/train_model.py` | Entra√Æne le mod√®le K-Means sur les donn√©es historiques. |
| **Processing** | `processing/processor.py` | Traite le flux Kafka en temps r√©el avec Spark Streaming. |
| **Visualization** | `visualization/dashboard.py` | Affiche les alertes et m√©triques en temps r√©el via Streamlit. |

---

## 1. Module Ingestion (`ingestion/producer.py`)
**R√¥le :** Simuleur de donn√©es IoT.
Ce script lit un fichier CSV de base, injecte al√©atoirement des anomalies (ex: incendie, fuite de gaz) et envoie les donn√©es au topic Kafka `iot_data`.

### Fonctionnalit√©s Cl√©s :
- Simulation de sc√©narios : `SURCHAUFFE`, `GEL`, `FUITE_GAZ`, `INCENDIE`, `HUMIDITE`.
- Envoi JSON vers Kafka toutes les 3 secondes.

### Exemple de Code (G√©n√©ration d'Anomalie) :
```python
# Extrait de producer.py
def create_anomaly_values(base_data):
    scenario = "INCENDIE"
    # Simule une temp√©rature extr√™me et de la fum√©e
    base_data['temp'] = random.uniform(150.0, 300.0)
    base_data['smoke'] = random.uniform(0.1, 0.5)
    return base_data
```

---

## 2. Module Machine Learning (`ml/train_model.py`)
**R√¥le :** Cr√©ation du mod√®le de d√©tection.
Ce script utilise PySpark pour entra√Æner un algorithme de clustering non-supervis√© (K-Means) qui apprendra ce qu'est un comportement "normal".

### Fonctionnalit√©s Cl√©s :
- Chargement des donn√©es depuis HDFS.
- Normalisation des features (`StandardScaler`).
- Entra√Ænement K-Means (`k=2`).
- Sauvegarde du mod√®le et du scaler sur HDFS pour l'inf√©rence.

### Exemple de Code 1 (Pr√©paration & Vectorisation) :
Avant l'entra√Ænement, il est crucial de pr√©parer les donn√©es.
1. **Conversion** : `cast(FloatType)` transforme le texte en nombres.
2. **Nettoyage** : `na.fill(0.0)` remplace les valeurs manquantes.
3. **Vectorisation** : `VectorAssembler` fusionne les 7 colonnes en un seul vecteur pour Spark ML.

```python
# Extrait de train_model.py
assembler = VectorAssembler(
    inputCols=['co', 'humidity', 'light', 'lpg', 'motion', 'smoke', 'temp'], 
    outputCol="raw_features"
)
df_vec = assembler.transform(df)
```

### Exemple de Code 2 (Entra√Ænement) :
```python
# Extrait de train_model.py
kmeans = KMeans(
    k=2, 
    featuresCol="features",
    predictionCol="cluster"
)
model = kmeans.fit(df_scaled)
model.save("hdfs://namenode:9000/iot/models/kmeans_iot")
```

---

## 3. Module Processing (`processing/processor.py`)
**R√¥le :** Moteur de d√©tection en temps r√©el.
Ce script Spark Structured Streaming consomme les messages Kafka, applique le mod√®le K-Means entra√Æn√©, et d√©termine si une donn√©e est anormale en calculant sa distance par rapport au centre du cluster.

### Fonctionnalit√©s Cl√©s :
- Calcul de distance (Score d'anomalie).
- Identification de la cause (ex: quelle sonde a une valeur aberrante).
- Stockage historique dans HDFS (Parquet).
- Mise √† jour d'un buffer CSV local pour le dashboard.

### Exemple de Code (Analyse) :
```python
# Extrait de processor.py
@udf(StringType())
def analyze_anomaly(features, prediction):
    dist = calculate_distance(features, prediction)
    if dist > ANOMALY_THRESHOLD:
        return f"ANOMALY|{dist:.2f}|TEMP_HIGH"
    return f"NORMAL|{dist:.2f}|RAS"
```

---

## 4. Module Visualization (`visualization/dashboard.py`)
**R√¥le :** Interface Utilisateur.
Une application Streamlit qui lit le buffer CSV mis √† jour par le Processeur pour afficher l'√©tat du syst√®me.

### Fonctionnalit√©s Cl√©s :
- M√©triques en temps r√©el (Temp√©rature, Score).
- Mise en √©vidence visuelle des anomalies (Rouge).
- Graphiques d'√©volution du score d'anomalie.

### Exemple de Code (Affichage) :
```python
# Extrait de dashboard.py
if status == 'ANOMALY':
    st.metric("√âtat", "CRITIQUE", delta_color="inverse")
    st.error(f"Cause: {cause}")
else:
    st.metric("√âtat", "NORMAL")
```
